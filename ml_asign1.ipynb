{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. What is a parameter?\n",
        "\n",
        "A parameter is an internal coefficient of a model that is learned automatically during training. It defines how the input data is transformed into output predictions.\n",
        "\n",
        "Every Machine Learning model works as a mathematical function. Parameters are the unknowns in this function that need to be estimated from data. During training, algorithms optimize parameters to minimize the loss function.\n",
        "\n",
        "Examples:\n",
        "In Linear Regression,\n",
        "y=w⋅x+b\n",
        "\n",
        "where w = slope (weight) and b = bias (intercept). Both are parameters.\n",
        "In Logistic Regression, parameters are weights of features in the sigmoid function.\n",
        "In Neural Networks, millions of parameters exist in the form of weights and biases.\n",
        "\n",
        "Importance:\n",
        "Parameters represent the knowledge learned from data.\n",
        "Well-trained parameters → accurate predictions.\n",
        "Difference from Hyperparameters:\n",
        "Parameters = learned by model (e.g., weights).\n",
        "Hyperparameters = set manually before training (e.g., learning rate, number of layers).\n",
        "\n",
        "2. What is correlation?\n",
        "\n",
        "Definition:\n",
        "Correlation is a statistical measure that expresses the strength and direction of a linear relationship between two variables.\n",
        "\n",
        "Range:\n",
        "+1 : Perfect positive correlation (variables rise together).\n",
        "-1 : Perfect negative correlation (one rises, the other falls).\n",
        "0 : No linear relationship.\n",
        "\n",
        "Applications in ML:\n",
        "Detect redundant features (high correlation).\n",
        "Feature selection (remove correlated variables to avoid multicollinearity).\n",
        "\n",
        "3. What does negative correlation mean?\n",
        "\n",
        "Definition:\n",
        "Negative correlation means there is an inverse relationship between two variables: as one increases, the other decreases.\n",
        "\n",
        "Example:Number of Study Hours and Number of TV Hours\n",
        "When the number of hours spent studying increases, the number of hours spent watching television usually decreases.\n",
        "This relationship represents a negative correlation.\n",
        "In statistics, negative correlation means that when one variable goes up, the other variable tends to go down.\n",
        "\n",
        "Interpretation in ML:\n",
        "If two features are negatively correlated, including both might confuse the model. Feature engineering may be required.\n",
        "\n",
        "4. Define Machine Learning. What are its main components?\n",
        "Machine Learning is a branch of AI that allows systems to learn from data, identify patterns, and make predictions or decisions without being explicitly programmed.\n",
        "\n",
        "Key Characteristics:\n",
        "Data-driven (requires datasets).\n",
        "Improves performance over time.\n",
        "Learns patterns automatically.\n",
        "\n",
        "Main Components:\n",
        "Data : Raw information collected for training.\n",
        "Features (X) : Independent input variables.\n",
        "Target (y) : Output variable to predict.\n",
        "Model/Algorithm : Mathematical/statistical representation used to map X → y.\n",
        "Loss Function : Evaluates prediction error.\n",
        "Optimizer : Algorithm to minimize loss (e.g., Gradient Descent).\n",
        "Evaluation Metrics : Used to judge model quality (accuracy, F1 score, RMSE).\n",
        "\n",
        "Example: Predicting house prices → Data = past house sales, Features = area, location, rooms, Target = price.\n",
        "\n",
        "5. How does loss value determine if a model is good?\n",
        "The loss function calculates how far predicted values are from actual values.\n",
        "\n",
        "Interpretation:\n",
        "Small loss → model predictions are close to real values → good model.\n",
        "Large loss → poor predictions → bad model.\n",
        "\n",
        "Common Loss Functions:\n",
        "Regression → Mean Squared Error (MSE), Mean Absolute Error (MAE).\n",
        "Classification → Cross-Entropy Loss.\n",
        "\n",
        "Example:\n",
        "Predict house price = ₹50,00,000\n",
        "Actual = ₹51,00,000\n",
        "Loss = 1,00,000 → small → good model.\n",
        "\n",
        "6. Continuous vs Categorical Variables\n",
        "\n",
        "Continuous Variables:\n",
        "Can take infinite values within a range.\n",
        "Examples: Temperature, salary, height.\n",
        "Measured on interval or ratio scales.\n",
        "Categorical Variables:\n",
        "Represent groups or categories.\n",
        "\n",
        "Two types:\n",
        "Nominal: No order (e.g., gender, blood group).\n",
        "Ordinal: With order (e.g., education level).\n",
        "In ML: Continuous values are used directly, categorical must be encoded.\n",
        "\n",
        "7. Handling categorical variables\n",
        "ML algorithms require numbers. Techniques:\n",
        "Label Encoding : Converts categories into integers.\n",
        "One-Hot Encoding : Creates binary variables.\n",
        "Target Encoding : Uses mean of target per category.\n",
        "Frequency Encoding : Uses frequency of categories.\n",
        "Impact: Choosing the wrong method can cause bias.\n",
        "\n",
        "8. Training vs Testing Dataset\n",
        "\n",
        "Training Set: Used to fit the model and learn parameters.\n",
        "Testing Set: Used to evaluate model generalization on unseen data.\n",
        "It prevents overfitting (memorizing training data but failing on new data).\n",
        "\n",
        "9. What is sklearn.preprocessing?\n",
        "A module in Scikit-learn providing tools for data transformation.\n",
        "Functions:\n",
        "Scaling → StandardScaler, MinMaxScaler.\n",
        "Encoding → LabelEncoder, OneHotEncoder.\n",
        "Normalization → Adjust feature magnitudes.\n",
        "Purpose: Ensures all features are in a suitable format for ML models.\n",
        "\n",
        "10. What is a Test set?\n",
        "A subset of data kept aside for final model evaluation.\n",
        "Purpose: Measures how well the trained model generalizes to unseen data.\n",
        "\n",
        "11. Data Splitting in Python\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "test_size=0.2 → 20% data for testing.\n",
        "Ensures fair evaluation.\n",
        "\n",
        "12. How do you approach an ML problem?\n",
        "\n",
        "Steps:\n",
        "Define the problem.\n",
        "Collect and understand data.\n",
        "Perform EDA.\n",
        "Preprocess (missing values, encoding, scaling).\n",
        "Split dataset.\n",
        "Select algorithm.\n",
        "Train model.\n",
        "Evaluate using metrics.\n",
        "Tune hyperparameters.\n",
        "Deploy & monitor.\n",
        "\n",
        "13. Why do EDA before model fitting?\n",
        "\n",
        "Detect missing data.\n",
        "Check feature distributions.\n",
        "Identify outliers.\n",
        "Detect correlations between variables.\n",
        "Helps in feature engineering.\n",
        "Without EDA, the model might learn from biased or incomplete data.\n",
        "\n",
        "14. Finding correlation in Python\n",
        "import pandas as pd\n",
        "df.corr()\n",
        "Visualization:\n",
        "import seaborn as sns\n",
        "sns.heatmap(df.corr(), annot=True, cmap=\"coolwarm\")\n",
        "\n",
        "15. What is causation? Difference from correlation\n",
        "\n",
        "Correlation: Two variables move together, but one may not cause the other.\n",
        "Causation: One variable directly influences another.\n",
        "\n",
        "Example:\n",
        "Ice cream sales increase and drowning increase (correlation).\n",
        "Heat increase causes both (causation).\n",
        "\n",
        "\n",
        "16. Optimizers in ML\n",
        "Definition: Algorithms that adjust model parameters to minimize loss.\n",
        "\n",
        "Types:\n",
        "Gradient Descent → Updates weights by slope of loss function.\n",
        "SGD : Updates for each sample.\n",
        "Adam : Adaptive learning rate, widely used.\n",
        "RMSProp : Uses moving average of squared gradients.\n",
        "Importance: Helps in fast and efficient convergence.\n",
        "\n",
        "17. sklearn.linear_model\n",
        "A scikit-learn module for linear models.\n",
        "Includes:\n",
        "LinearRegression\n",
        "LogisticRegression\n",
        "Ridge, Lasso, ElasticNet\n",
        "\n",
        "18. model.fit()\n",
        "Trains the model on given training data.\n",
        "\n",
        "Arguments:\n",
        "X_train → Features.\n",
        "y_train → Labels.\n",
        "\n",
        "19. model.predict()\n",
        "Uses trained model to make predictions.\n",
        "Arguments: X_test → Features for prediction.\n",
        "\n",
        "20. Feature Scaling\n",
        "Process of adjusting values of features to a common scale.\n",
        "It prevents domination of large features.\n",
        "Speeds up convergence in gradient descent.\n",
        "\n",
        "Methods:\n",
        "Standardization (Z-score).\n",
        "Normalization (0–1 range).\n",
        "\n",
        "21. Scaling in Python\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "X_normalized = scaler.fit_transform(X)\n",
        "\n",
        "22. Data Encoding\n",
        "Converting categorical variables into numeric values.\n",
        "Methods: Label Encoding, One-Hot Encoding, Target Encoding.\n",
        "Importance: Essential since ML algorithms require numerical input."
      ],
      "metadata": {
        "id": "lnyl6mT358Al"
      }
    }
  ]
}